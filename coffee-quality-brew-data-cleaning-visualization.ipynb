{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/jyotsnacd/coffee-quality-brew-data-cleaning-visualization?scriptVersionId=133718173\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\npd.set_option('display.max_columns', 50)\npd.set_option('display.width', 50)\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\nfrom scipy import stats\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-06-13T14:17:27.950532Z","iopub.execute_input":"2023-06-13T14:17:27.950903Z","iopub.status.idle":"2023-06-13T14:17:27.968907Z","shell.execute_reply.started":"2023-06-13T14:17:27.950876Z","shell.execute_reply":"2023-06-13T14:17:27.967955Z"},"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Loading Data and checking samples**","metadata":{}},{"cell_type":"code","source":"filepath = \"/kaggle/input/coffee-quality-data-cqi/df_arabica_clean.csv\"\ndf = pd.read_csv(filepath)\ndf.head(3)","metadata":{"execution":{"iopub.status.busy":"2023-06-13T14:45:24.508585Z","iopub.execute_input":"2023-06-13T14:45:24.509391Z","iopub.status.idle":"2023-06-13T14:45:24.558589Z","shell.execute_reply.started":"2023-06-13T14:45:24.509352Z","shell.execute_reply":"2023-06-13T14:45:24.557404Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# making ID as DF index and removing Unnamed columns\ndf.set_index(\"ID\",inplace = True)\ndf.drop(\"Unnamed: 0\",axis=1,inplace=True)\n","metadata":{"execution":{"iopub.status.busy":"2023-06-13T14:17:41.307129Z","iopub.execute_input":"2023-06-13T14:17:41.307523Z","iopub.status.idle":"2023-06-13T14:17:41.316197Z","shell.execute_reply.started":"2023-06-13T14:17:41.307493Z","shell.execute_reply":"2023-06-13T14:17:41.314817Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Let's all see how categorical variales are loooking\ndf.describe(include=\"object\")","metadata":{"execution":{"iopub.status.busy":"2023-06-13T14:17:57.658001Z","iopub.execute_input":"2023-06-13T14:17:57.65883Z","iopub.status.idle":"2023-06-13T14:17:57.711507Z","shell.execute_reply.started":"2023-06-13T14:17:57.658787Z","shell.execute_reply":"2023-06-13T14:17:57.710363Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"###  Observations\n1. The columns \"Company\" and \"Owner\" provide redundant information, with the additional inclusion of Chinese names in the \"Owner\" column. Therefore,   it is safe to remove the \"Owner\" column from the dataset. <br>\n2. Bag weight seems to be numeric.\"kg\" suffix need to be removed and data type should be changed it float  <br>\n3. [\"Producer\",\"In-Country Partner\",\"Certification Body\",\"Farm Name\",\"Mill\",\"Region\"] have some chinese character. They need cleaning.<br>\n4. \"Status\" column has no information. So we can drop it.<br>\n5. \"Grading Date\",\"Expiration\",\"Harvest Year\" need to be converted in Date Format.<br>\n6. \"Certification Address\" and \"Certification Contact\" can be deleted as we have region column and contact person seems irrelevant here.<br>\n7. \"Harvest Year\" sometimes have values in the form of ranges. We can keep the first value in order to have the values of columns as a single year.<br>\n8. As we can see that more than half of ICO Number is Null, so we can straightaway drop the column.\n","metadata":{}},{"cell_type":"markdown","source":"\n","metadata":{}},{"cell_type":"code","source":"#removing columns \"Owner\", \"Certification Address\", \"Certification Contact\", \"Status\"\ntry:\n    df.drop([\"Owner\", \"Certification Address\", \"Certification Contact\", \"Status\",\"Lot Number\",\"ICO Number\"],axis=1, inplace=True)\nexcept KeyError:\n    pass\n\n# removing kg from Bag weight column and converting to int dtype\ntry:\n    df[\"Bag Weight\"] = df[\"Bag Weight\"].apply(lambda x: x.split(\" \")[0]).astype(\"int64\")\nexcept AttributeError:\n    pass\n\n# removing chinese character from \"In-company Partner\" and \"Producer\"\nimport re \n#to extract only english characters\ndef extract_eng_char(s):\n    s = str(s)\n    s = re.sub(r'[^a-zA-Z0-9\\s]', '', s).strip()\n    return s\n\ncolumns_with_noneng = [\"Producer\",\"In-Country Partner\",\"Certification Body\",\"Farm Name\",\"Mill\",\"Region\"]\nfor col in columns_with_noneng:\n    df[col] = df[col].apply(extract_eng_char)\n    #Set unknown as producer for empty strings\n    df[col] = df[col].replace(\"\",\"unknown\")\n\n\n#let's keep a copy of df so far\ndf_1 = df.copy()\n\n#\"Expiration\" needs to be converted in Date Format.\ndate_cols = [\"Grading Date\",\"Expiration\"]\nfor col in date_cols:\n    df[col] = pd.to_datetime(df[col])\n    \n#Keeping first value of harvest value\ndf[\"Harvest Year\"] = df[\"Harvest Year\"].apply(lambda x: x.strip().split(\"/\")[0].strip() if \"/\" in x.strip() else x.strip())\ndf[\"Harvest Year\"] = pd.to_datetime(df[\"Harvest Year\"],format=\"%Y\")\n\n\n","metadata":{"execution":{"iopub.status.busy":"2023-06-13T15:02:25.256649Z","iopub.execute_input":"2023-06-13T15:02:25.257058Z","iopub.status.idle":"2023-06-13T15:02:25.26298Z","shell.execute_reply.started":"2023-06-13T15:02:25.257028Z","shell.execute_reply":"2023-06-13T15:02:25.261725Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* Let's see all the numerical and categorical columns","metadata":{}},{"cell_type":"code","source":"#drop columns with unique value = 1\ncols_to_drop = df.columns[df.nunique()==1]\ndf.drop(cols_to_drop,axis=1, inplace = True)","metadata":{"execution":{"iopub.status.busy":"2023-06-13T14:18:22.634053Z","iopub.execute_input":"2023-06-13T14:18:22.634457Z","iopub.status.idle":"2023-06-13T14:18:22.646252Z","shell.execute_reply.started":"2023-06-13T14:18:22.634426Z","shell.execute_reply":"2023-06-13T14:18:22.645051Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.describe(include=\"object\")","metadata":{"execution":{"iopub.status.busy":"2023-06-13T14:18:25.310026Z","iopub.execute_input":"2023-06-13T14:18:25.310497Z","iopub.status.idle":"2023-06-13T14:18:25.345235Z","shell.execute_reply.started":"2023-06-13T14:18:25.310462Z","shell.execute_reply":"2023-06-13T14:18:25.344413Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Observations in Categorical Columns**\n* Among all, we can observe that [\"Farm Name\", \"Mill\", \"Region\", \"Producer\"] have majority values as unknown. However, let's not discard it right away.\n* Except for Color and Processing Methods, there seems to be too many categories of each column. We can later check if we can merge under-represented column values\n*Altitude appears to be a numerical column.and there are also some range altitudes. let's take the average and convert the dtype to int right away","metadata":{}},{"cell_type":"code","source":"#Calculating Average Altitude where range is given\ndef get_avg_alt(x):\n    x = str(x)\n    x = re.findall(r'\\d+|\\D+', x)\n    if len(x) ==3:\n        return (int(x[0])+int(x[2]))//2\n    elif len(x) == 1 and x[0].isdigit():\n        return int(x[0])\n    else:\n        return -1\ndf[\"Altitude\"] = df[\"Altitude\"].apply(get_avg_alt)\n\n#As we can see \"Processing Method\", \"Variety\" have a very few missing values. \ndf[\"Processing Method\"] = df[\"Processing Method\"].fillna(\"Unknown\")\ndf[\"Variety\"] = df[\"Variety\"].fillna(\"Unknown\")","metadata":{"execution":{"iopub.status.busy":"2023-06-13T14:18:27.421712Z","iopub.execute_input":"2023-06-13T14:18:27.422162Z","iopub.status.idle":"2023-06-13T14:18:27.433351Z","shell.execute_reply.started":"2023-06-13T14:18:27.422131Z","shell.execute_reply":"2023-06-13T14:18:27.432005Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"numerical_cols = df.select_dtypes(include=['float', 'int']).columns\nnon_num_cols = df.select_dtypes(include=['object']).columns\ndate_cols = df.select_dtypes(include=['datetime64']).columns\n\nprint(numerical_cols,end=\"\\n\\n\")\nprint(non_num_cols,end=\"\\n\\n\")\nprint(date_cols,end=\"\\n\")","metadata":{"execution":{"iopub.status.busy":"2023-06-13T14:18:41.751618Z","iopub.execute_input":"2023-06-13T14:18:41.752006Z","iopub.status.idle":"2023-06-13T14:18:41.76082Z","shell.execute_reply.started":"2023-06-13T14:18:41.751977Z","shell.execute_reply":"2023-06-13T14:18:41.759689Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Numerical Variables","metadata":{}},{"cell_type":"code","source":"#Visualizing Distributions for numerical coulmns\n\nfig,axs = plt.subplots(4,4,figsize=(12,12))\nplt.subplots_adjust(wspace=0.8,hspace=0.8)\n\nfor i,col in enumerate(numerical_cols):\n    sns.histplot(df[col],color=\"skyblue\", ax = axs[int(i/4),i%4]).set_ylabel(\"\")\nplt.xticks(rotation=45)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-06-13T14:18:43.746439Z","iopub.execute_input":"2023-06-13T14:18:43.746826Z","iopub.status.idle":"2023-06-13T14:18:47.997119Z","shell.execute_reply.started":"2023-06-13T14:18:43.746799Z","shell.execute_reply":"2023-06-13T14:18:47.996262Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Observations:**\n* \"Aroma\", \"Flavor\", \"Aftertaste\", \"Acidity\", \"Body\", \"Balance\" and \"Total Cups\" appear to be normally distributed. We can check for normality test\n* \"Moisture Percentage\" and \"Altitude\" seem to be normally distributed too except for a few outliers.\n* \"Category Two defects\" are exponential distributed\n* \"Uniformity\" and \"Catgory One Defects\" have majority as a single value except for a few outliers. So we can discard these columns considering they don't carry any useful information.\n* \"Bag Weight\" seems to have outliers. hence we need to remove them.","metadata":{}},{"cell_type":"code","source":"#First Let's perform the normality Test (\"Shapiro-Wilk Test\") to confirm the normality of columns:\n#[\"Aroma\", \"Flavor\", \"Aftertaste\", \"Acidity\", \"Body\", \"Balance\" and \"Total Cups\"]\n\ncols_to_test = [\"Aroma\", \"Flavor\", \"Aftertaste\", \"Acidity\", \"Body\", \"Balance\" and \"Total Cup Points\", \"Moisture Percentage\", \"Altitude\"]\n\nfor col in cols_to_test:\n    statistic, p_value = stats.shapiro(df[col])\n    # Check the p-value\n    alpha = 0.05  # Set the significance level\n    if p_value > alpha:\n        print(f\"{col} values are normally distributed (failed to reject H0)\",end=\"\\n\")\n    else:\n        print(f\"{col} values are not normally distributed (may reject H0)\",end=\"\\n\")\n","metadata":{"execution":{"iopub.status.busy":"2023-06-13T14:18:47.998577Z","iopub.execute_input":"2023-06-13T14:18:47.999108Z","iopub.status.idle":"2023-06-13T14:18:48.006597Z","shell.execute_reply.started":"2023-06-13T14:18:47.999078Z","shell.execute_reply":"2023-06-13T14:18:48.005813Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Results: **None of the above columns follow Normal Distribution except \"Total Cup Points\"**","metadata":{}},{"cell_type":"code","source":"# Let's now handle the outliers in the aforementioned columns : \"Bag Weight\", \"Moisture Percentage\" and \"Altitude\"\n#Let's replace the outliers with mean values. We can assume lower and upper bound thresholds of outliers as 5th percentile and 95th percentile values\n\ncols_with_outliers = [\"Bag Weight\", \"Moisture Percentage\", \"Altitude\",\"Aroma\",\"Quakers\", \"Balance\", \"Number of Bags\"]\nfor col in cols_with_outliers:\n    lower_thresh = np.percentile(df[col],5)\n    upper_thresh = np.percentile(df[col],95)\n    mean_val = np.mean(df[col])\n    df.loc[(df[col] <= lower_thresh) | (df[col] >= upper_thresh), col] = mean_val\n\n#Also, dropping columns \"Uniformity\" and \"Catgory One Defects\" since they don't carry any useful info\n# df.drop(\"Uniformity\",axis=1,inplace=True)\ncols_to_drop = [\"Uniformity\", \"Category One Defects\"]\n#df.drop(\"Category One Defects\",axis=1,inplace=True)\n\n\nnumerical_cols = numerical_cols[(numerical_cols!=\"Uniformity\") & (numerical_cols!=\"Category One Defects\")]\n#numerical_cols.remove([\"Category One Defects\"],axis=1, inplace=True)\n    ","metadata":{"execution":{"iopub.status.busy":"2023-06-13T14:18:56.462456Z","iopub.execute_input":"2023-06-13T14:18:56.46305Z","iopub.status.idle":"2023-06-13T14:18:56.479077Z","shell.execute_reply.started":"2023-06-13T14:18:56.463018Z","shell.execute_reply":"2023-06-13T14:18:56.478053Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Visualizing Distributions after handling outliers\n\nfig,axs = plt.subplots(4,4,figsize=(12,12))\nplt.subplots_adjust(wspace=0.8,hspace=0.8)\n\nfor i,col in enumerate(numerical_cols):\n    sns.histplot(df[col],color=\"skyblue\", ax = axs[int(i/4),i%4]).set_ylabel(\"\")\nfig.delaxes(axs[3, 2])\nfig.delaxes(axs[3, 3])\nplt.xticks(rotation=45)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-06-13T14:19:04.336651Z","iopub.execute_input":"2023-06-13T14:19:04.337064Z","iopub.status.idle":"2023-06-13T14:19:06.433084Z","shell.execute_reply.started":"2023-06-13T14:19:04.337031Z","shell.execute_reply":"2023-06-13T14:19:06.432273Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Pre-processing Catgorical Variables","metadata":{}},{"cell_type":"code","source":"#Let's check the summary statistics of Categorical variables\ndf.describe(include=\"object\")","metadata":{"execution":{"iopub.status.busy":"2023-06-13T14:19:10.723549Z","iopub.execute_input":"2023-06-13T14:19:10.724171Z","iopub.status.idle":"2023-06-13T14:19:10.756155Z","shell.execute_reply.started":"2023-06-13T14:19:10.724135Z","shell.execute_reply":"2023-06-13T14:19:10.755328Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Categorical Variables Distribution\nfig,axs = plt.subplots(4,3,figsize=(12,12))\nplt.subplots_adjust(wspace=0.8,hspace=0.8)\n\nfor i,col in enumerate(non_num_cols):\n    sns.countplot(x=col,data=df,color=\"skyblue\", ax = axs[int(i/3),i%3]).set_ylabel(\"\")\n    \n    \n# Hide x-ticks for each subplot\nfor ax in axs.flat:\n    ax.set_xticks([])\nfig.delaxes(axs[3, 2])\nplt.xticks([])\nplt.show()\n\n\n","metadata":{"execution":{"iopub.status.busy":"2023-06-13T14:19:21.187063Z","iopub.execute_input":"2023-06-13T14:19:21.187478Z","iopub.status.idle":"2023-06-13T14:19:24.836307Z","shell.execute_reply.started":"2023-06-13T14:19:21.187447Z","shell.execute_reply":"2023-06-13T14:19:24.835546Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Observations:**\n* Each of the Categorical Variable has one or two dominating values  while the other values represent a smaller proportion of data.\n* \"Farmname\", \"Mill\", \"Region\" and \"Producer\" have missing values/chinese characters which we had replaced with \"Unknown\" earlier while cleaning the data\n* We can combine the minority elements into a single category value in the columns where data is highly imbalanced, or we can also consider setting a lower threshold count to combine categories","metadata":{}},{"cell_type":"code","source":"#Combining categories for each column where represent less than 10% of datapoints\nthreshold = 5\nfor col in non_num_cols:\n    col_freq = df[col].value_counts()\n    to_be_changed = col_freq[col_freq<5].index\n    df[col] = df[col].replace(to_be_changed,\"Others\")\n    \n    ","metadata":{"execution":{"iopub.status.busy":"2023-06-13T14:19:28.284873Z","iopub.execute_input":"2023-06-13T14:19:28.28558Z","iopub.status.idle":"2023-06-13T14:19:28.364224Z","shell.execute_reply.started":"2023-06-13T14:19:28.28554Z","shell.execute_reply":"2023-06-13T14:19:28.363211Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Categorical Variables Distribution\nfig,axs = plt.subplots(4,3,figsize=(16,16))\nplt.subplots_adjust(wspace=0.8,hspace=0.8)\n\nfor i,col in enumerate(non_num_cols):\n    cat_sorted = df[col].value_counts().index\n    sns.countplot(x=col,data=df,color=\"skyblue\", ax = axs[int(i/3),i%3], order = cat_sorted ).set_ylabel(\"\")\n    \n    \nfor ax in axs.flat:\n    ax.set_xticklabels(ax.get_xticklabels(), rotation=45,fontsize=5)\n    \nfig.delaxes(axs[3, 2])\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-06-13T14:43:58.922597Z","iopub.execute_input":"2023-06-13T14:43:58.922996Z","iopub.status.idle":"2023-06-13T14:44:00.636704Z","shell.execute_reply.started":"2023-06-13T14:43:58.922965Z","shell.execute_reply":"2023-06-13T14:44:00.635308Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Visualizing the correlation between numerical features\ncorr_matrix = df[numerical_cols].corr()\nplt.figure(figsize=(8, 6))\nsns.heatmap(corr_matrix, annot=True, cmap='coolwarm', linewidths=0.5)","metadata":{"execution":{"iopub.status.busy":"2023-06-13T14:22:32.083825Z","iopub.execute_input":"2023-06-13T14:22:32.084223Z","iopub.status.idle":"2023-06-13T14:22:33.070475Z","shell.execute_reply.started":"2023-06-13T14:22:32.084192Z","shell.execute_reply":"2023-06-13T14:22:33.069539Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Observations:\n* It is evident that there exists a strong correlation among the properties that define the quality of coffee.\n* By examining the correlation between \"Total Cup Points\" and various characteristics of coffee, we can gain insights into the relative importance assigned to each quality test when determining the points for the overall quality evaluation.","metadata":{}},{"cell_type":"markdown","source":"# Conclusion: <br>\n**1. The dataset offers a comprehensive overview of the necessary data for analyzing coffee quality, encompassing all relevant coffee properties and   their correlations, thereby providing a holistic perspective.** <br>\n**2. The categorical columns in the dataset exhibit a significant imbalance, with a small number of values dominating the majority of data points.**<br>\n**3. After performing the Shapiro-Wilk test, it was found that only the \"Total Cup Points\" column follows a normal distribution, while the other numerical columns deviate from normality even though they look approximately normal.**<br>\n**4. It is necessary to address outliers and perform data cleaning for the numerical columns.**<br>\n**5. Certain columns, such as \"Uniformity\" and \"Category One Defects,\" contain only a single value with some outliers, making them uninformative. As a result, these columns can be safely discarded from the analysis.**<br>\n\n","metadata":{}},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"markdown","source":"*PS - If you appreciate the work in my notebook, I kindly request you to consider upvoting it. Your support would be greatly appreciated. Thank you!*","metadata":{}},{"cell_type":"markdown","source":"","metadata":{}}]}